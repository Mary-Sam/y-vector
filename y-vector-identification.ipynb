{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorch-tdnn","metadata":{"execution":{"iopub.status.busy":"2022-04-08T04:43:47.968796Z","iopub.execute_input":"2022-04-08T04:43:47.969506Z","iopub.status.idle":"2022-04-08T04:43:58.909293Z","shell.execute_reply.started":"2022-04-08T04:43:47.969355Z","shell.execute_reply":"2022-04-08T04:43:58.908441Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nimport pandas as pd\n\nimport librosa \nimport librosa.display\nimport IPython.display as ipd\nfrom tqdm.notebook import tqdm\nimport pickle\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nimport torch.nn.functional as F\nfrom torch import nn\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder \nimport glob\nimport random\n\nfrom sklearn.metrics import accuracy_score\nfrom pytorch_tdnn.tdnn import TDNN as TDNNLayer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-08T04:43:58.913055Z","iopub.execute_input":"2022-04-08T04:43:58.913277Z","iopub.status.idle":"2022-04-08T04:44:02.411368Z","shell.execute_reply.started":"2022-04-08T04:43:58.913251Z","shell.execute_reply":"2022-04-08T04:44:02.410550Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# tf-SE Downsampling Block\nclass tfSE(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        \n        # frequence re-calibration - частотное преобразование\n        self.freq_recal = nn.Sequential(nn.Linear(channels, channels), nn.Sigmoid())\n        \n        # time re-calibration - временное преобразование\n        self.time_recal = nn.Sequential(nn.Linear(channels, 1), nn.Sigmoid())\n\n    def forward(self, x):\n        \n        # формула 2\n        avgpool = nn.AvgPool1d(x.shape[-1], stride=1)\n        freq_recal = self.freq_recal(avgpool(x).squeeze(-1))\n        x = x * freq_recal.unsqueeze(-1)\n        \n        # формула 3\n        xt = self.time_recal(x.permute(0, 2, 1)).T.permute(2, 0, 1)\n        out = xt * x\n\n        return out\n\nclass WaveformEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        def baselayer(n_in, n_out, k, stride, padding=0):\n            return nn.Sequential(\n                nn.Conv1d(n_in, n_out, k, stride=stride, bias=False, padding=padding),\n                nn.Dropout(p=0.0),\n                nn.InstanceNorm1d(n_out, affine=False),\n                nn.ReLU()\n            )\n        \n        # 3 параллельных канала фильтров\n        self.wavefilter = nn.ModuleList().extend([\n                nn.Sequential(\n                    baselayer(1, 90, 36, 18, 0), \n                    baselayer(90, 192, 5, 1, 2)\n                ),\n                nn.Sequential(\n                    baselayer(1, 90, 18, 9, 0), \n                    baselayer(90, 160, 5, 2, 0)\n                ),\n                nn.Sequential(\n                    baselayer(1, 90, 12, 6, 0),\n                    baselayer(90, 160, 5, 3, 0)\n                )\n        ])\n              \n        self.skip1 = nn.MaxPool1d(kernel_size=5, stride=8)\n        self.skip2 = nn.MaxPool1d(kernel_size=3, stride=4, padding=1)\n        self.skip3 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n        \n        self.conv1 = baselayer(512, 512, 5, 2)\n        self.conv2 = baselayer(512, 512, 3, 2)\n        self.conv3 = baselayer(512, 512, 3, 2, padding=2)\n        \n        self.am1 = tfSE(512)\n        self.am2 = tfSE(512)\n        self.am3 = tfSE(512)\n        self.am4 = tfSE(512*4)\n\n    def forward(self, x):\n        enc = []\n        ft_shape = []\n        \n        # Multiscale filtering layer\n        for conv in self.wavefilter:\n            enc.append(conv(x))\n            ft_shape.append(conv(x).shape[-1])\n            \n        #Выбираем наименьшую длину и конкатенируем по ней\n        ft_max = np.min(np.array(ft_shape))\n        enc = torch.cat((enc[0][:, :, :ft_max], enc[1][:, :, :ft_max], enc[2][:, :, :ft_max]), dim=1)\n        \n        #  Multi-level Feature Map Aggregation\n        skip1_out = self.skip1(enc)\n        out = self.conv1(enc)\n        out = self.am1(out)\n        skip2_out = self.skip2(out)\n        out = self.conv2(out)\n        out = self.am2(out)\n        skip3_out = self.skip3(out)\n        out = self.conv3(out)\n        out = self.am3(out)\n        \n        t_max = np.min(np.array([skip1_out.shape[-1], skip2_out.shape[-1], skip3_out.shape[-1], out.shape[-1]]))\n        out = torch.cat((skip1_out[:, :, :t_max], skip2_out[:, :, :t_max], skip3_out[:, :, :t_max], out[:, :, :t_max]), dim=1)\n        out = self.am4(out)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2022-04-08T04:44:02.412723Z","iopub.execute_input":"2022-04-08T04:44:02.412997Z","iopub.status.idle":"2022-04-08T04:44:02.435601Z","shell.execute_reply.started":"2022-04-08T04:44:02.412961Z","shell.execute_reply":"2022-04-08T04:44:02.433356Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Frame aggregator\nclass Aggregator(nn.Module):\n    def __init__(self, feature_dim=2048, wavlength=535, num_classes=109, embed_dim=128, p_dropout=0.0):\n        super(Aggregator, self).__init__()\n        \n        self.tdnn5 = TDNNLayer(feature_dim, 512, [-2,-1,0,1,2])\n        self.tdnn3 = TDNNLayer(512, 512, [-2,0,2])\n        self.tdnn1 = TDNNLayer(512, 512, [0])\n        self.tdnn_out = TDNNLayer(512, 1500, [0])\n        \n        self.layer_norm = nn.LayerNorm(wavlength)\n        self.relu = nn.ReLU()\n        \n        self.fc1 = nn.Linear(3000, 512)\n        self.bn = nn.BatchNorm1d(512)\n        self.dropout_fc1 = nn.Dropout(p=p_dropout)\n        self.lrelu = nn.LeakyReLU(0.2)\n        self.fc2 = nn.Linear(512, embed_dim)\n        self.fc3 = nn.Linear(embed_dim, num_classes)\n        \n    def forward(self, x):\n\n        x = self.tdnn5(x)\n        x = self.layer_norm(x)\n        x = self.relu(x)\n        \n        x = self.tdnn3(x)\n        x = self.layer_norm(x)\n        x = self.relu(x)\n        \n        x = self.tdnn3(x)\n        x = self.layer_norm(x)\n        x = self.relu(x)\n        \n        x = self.tdnn1(x)\n        x = self.layer_norm(x)\n        x = self.relu(x)\n        \n        x = self.tdnn_out(x)\n        x = self.layer_norm(x)\n        x = self.relu(x)\n               \n        stats = torch.cat((x.mean(dim=2), x.std(dim=2)), dim=1)\n               \n        x = self.fc1(stats)\n               \n        x = self.bn(x)\n        x = self.lrelu(x)\n        x = self.dropout_fc1(x)\n        x = self.fc2(x)\n        x = self.lrelu(x)\n        x = self.dropout_fc1(x)\n        x = self.fc3(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2022-04-08T04:44:02.438101Z","iopub.execute_input":"2022-04-08T04:44:02.438499Z","iopub.status.idle":"2022-04-08T04:44:02.454616Z","shell.execute_reply.started":"2022-04-08T04:44:02.438459Z","shell.execute_reply":"2022-04-08T04:44:02.453862Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self):\n        super(CustomModel, self).__init__()\n        \n        self.waveenc = WaveformEncoder()\n        self.agg = Aggregator()\n        \n    def forward(self, x, labels=None):\n        \n        x = self.waveenc(x)\n        x = self.agg(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2022-04-08T04:44:02.457523Z","iopub.execute_input":"2022-04-08T04:44:02.457888Z","iopub.status.idle":"2022-04-08T04:44:02.466471Z","shell.execute_reply.started":"2022-04-08T04:44:02.457849Z","shell.execute_reply":"2022-04-08T04:44:02.465685Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, path_wav, y, time_lenght=3.5):\n        super(CustomDataset).__init__()\n        self.y = y\n        self.path_wav = path_wav\n        self.time_lenght = time_lenght\n        \n    def __len__(self):\n        return len(self.path_wav)\n\n    def __getitem__(self, idx):\n        y_idx = self.y[idx]\n        wav, sr = librosa.load(self.path_wav[idx])\n        random_segment = self.get_random_lenght_segment(wav, sr)\n        return torch.Tensor(random_segment), torch.tensor(y_idx, dtype=torch.long), self.path_wav[idx]\n        \n    def get_random_lenght_segment(self, wav, sr):\n        segment_length = int(self.time_lenght * sr)\n        while len(wav) < segment_length:\n            wav = np.hstack((wav, wav))\n        return wav[-segment_length:]","metadata":{"execution":{"iopub.status.busy":"2022-04-08T04:44:02.468065Z","iopub.execute_input":"2022-04-08T04:44:02.468402Z","iopub.status.idle":"2022-04-08T04:44:02.477493Z","shell.execute_reply.started":"2022-04-08T04:44:02.468367Z","shell.execute_reply":"2022-04-08T04:44:02.476701Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Загрузим корпус VCTK","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/english-multispeaker-corpus-for-voice-cloning/VCTK-Corpus/VCTK-Corpus/wav48'\nfilepath = glob.glob(path + \"/**/*\")\nfilepath.remove('/kaggle/input/english-multispeaker-corpus-for-voice-cloning/VCTK-Corpus/VCTK-Corpus/wav48/p376/p376_295.raw')\nlabel = [x[-12:-8] for x in tqdm(filepath)]","metadata":{"execution":{"iopub.status.busy":"2022-04-08T04:44:02.478910Z","iopub.execute_input":"2022-04-08T04:44:02.479286Z","iopub.status.idle":"2022-04-08T04:44:12.055487Z","shell.execute_reply.started":"2022-04-08T04:44:02.479247Z","shell.execute_reply":"2022-04-08T04:44:12.054681Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"y = LabelEncoder().fit_transform(label)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T04:44:12.056789Z","iopub.execute_input":"2022-04-08T04:44:12.057628Z","iopub.status.idle":"2022-04-08T04:44:12.084093Z","shell.execute_reply.started":"2022-04-08T04:44:12.057584Z","shell.execute_reply":"2022-04-08T04:44:12.083259Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Разделим данные на обучение и валидации в пропорции 80/20","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(filepath, y, shuffle=True, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T04:44:12.087216Z","iopub.execute_input":"2022-04-08T04:44:12.087433Z","iopub.status.idle":"2022-04-08T04:44:12.109405Z","shell.execute_reply.started":"2022-04-08T04:44:12.087405Z","shell.execute_reply":"2022-04-08T04:44:12.108543Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_dataset = CustomDataset(X_train, y_train)\nval_dataset = CustomDataset(X_val, y_val)\n\nbatch_size = 64\ntrain_dataloader = DataLoader(train_dataset,\n                              sampler=RandomSampler(train_dataset),\n                              batch_size=batch_size,\n                              drop_last=True\n                              )\nval_dataloader = DataLoader(val_dataset,\n                            sampler=SequentialSampler(val_dataset),\n                            batch_size=batch_size,\n                           drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T04:44:12.112660Z","iopub.execute_input":"2022-04-08T04:44:12.113503Z","iopub.status.idle":"2022-04-08T04:44:12.120433Z","shell.execute_reply.started":"2022-04-08T04:44:12.113452Z","shell.execute_reply":"2022-04-08T04:44:12.119586Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = CustomModel().to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T04:44:12.121807Z","iopub.execute_input":"2022-04-08T04:44:12.122387Z","iopub.status.idle":"2022-04-08T04:44:15.596325Z","shell.execute_reply.started":"2022-04-08T04:44:12.122345Z","shell.execute_reply":"2022-04-08T04:44:15.595540Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load('../input/model-3ep/model_3ep.pkl'))","metadata":{"execution":{"iopub.status.busy":"2022-04-08T04:44:38.285475Z","iopub.execute_input":"2022-04-08T04:44:38.285769Z","iopub.status.idle":"2022-04-08T04:44:39.481535Z","shell.execute_reply.started":"2022-04-08T04:44:38.285734Z","shell.execute_reply":"2022-04-08T04:44:39.480750Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def learning_curves(train, val, txt='loss'):\n    plt.figure(figsize=(8,6))\n    plt.plot(range(1, len(train)+1), train, label='train {}'.format(txt))\n    plt.plot(range(1, len(val)+1), val, label='validation {}'.format(txt))\n    plt.title('Training history', fontsize=14)\n    plt.ylabel('{}'.format(txt), fontsize=14)\n    plt.xlabel('Epoch', fontsize=14)\n    plt.legend(fontsize=14)\n    plt.tick_params(axis='both', which='major', labelsize=14)\n    plt.grid()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T04:44:46.372793Z","iopub.execute_input":"2022-04-08T04:44:46.373401Z","iopub.status.idle":"2022-04-08T04:44:46.382727Z","shell.execute_reply.started":"2022-04-08T04:44:46.373364Z","shell.execute_reply":"2022-04-08T04:44:46.381870Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def train(train_dataloader, val_dataloader, model, optimizer, loss_fn, epoch):\n    losses_train = []\n    losses_val = []\n    accuracy_train = []\n    accuracy_val = []\n    EER_train = []\n    EER_val = []\n    for i in range(epoch):\n        loss_train = []\n        acc_train = []\n        eer_train = []\n        eer_val = []\n        loss_val = []\n        acc_val = []\n        m = nn.Softmax(dim=1)\n        model.train()\n        for d in tqdm(train_dataloader):\n            optimizer.zero_grad()\n            X = d[0].float().unsqueeze(1).to(device)\n            y = d[1].to(device)\n            predict = model(X)\n            logits = m(predict).detach().cpu().numpy().argmax(axis=-1).flatten().tolist()\n            loss = loss_fn(predict, y)\n            loss_train.append(loss.item())\n            acc_train.append(accuracy_score(logits, y.cpu().numpy()))\n            loss.backward()\n            optimizer.step()\n        if i % 1 == 0:\n            model.eval()\n            with torch.no_grad():\n                optimizer.zero_grad()\n                for d_v in tqdm(val_dataloader):\n                    X_val = d_v[0].unsqueeze(1).float().to(device)\n                    y_val = d_v[1].to(device)\n                    predict_val = model(X_val)\n                    loss_v = loss_fn(predict_val, y_val)\n                    logits_val = m(predict_val).detach().cpu().numpy().argmax(axis=-1).flatten().tolist()\n                    loss_val.append(loss_v.item())\n                    acc_val.append(accuracy_score(logits_val, y_val.cpu().numpy()))\n\n            print('')\n            print('---Train---')\n            print('Epoch: {}, Accuracy: {}, Loss: {}'.format(i, np.mean(acc_train), np.mean(loss_train)))\n            print('---Validation---')\n            print('Epoch: {}, Accuracy: {}, Loss: {}'.format(i, np.mean(acc_val), np.mean(loss_val)))\n            print('')\n            torch.save(model.state_dict(), f\"model_4ep.pkl\")\n        losses_train.append(np.mean(loss_train))\n        losses_val.append(np.mean(loss_val))\n        accuracy_train.append(np.mean(acc_train))\n        accuracy_val.append(np.mean(acc_val))\n    return {'losses_train': losses_train,\n           'losses_val': losses_val,\n           'accuracy_train': accuracy_train,\n           'accuracy_val': accuracy_val\n            }","metadata":{"execution":{"iopub.status.busy":"2022-04-08T04:44:59.701034Z","iopub.execute_input":"2022-04-08T04:44:59.701313Z","iopub.status.idle":"2022-04-08T04:44:59.719393Z","shell.execute_reply.started":"2022-04-08T04:44:59.701283Z","shell.execute_reply":"2022-04-08T04:44:59.717560Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"epoch = 1\ndct_history = train(train_dataloader, val_dataloader, model, optimizer, criterion, epoch)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T04:45:07.380200Z","iopub.execute_input":"2022-04-08T04:45:07.380464Z","iopub.status.idle":"2022-04-08T06:46:15.225644Z","shell.execute_reply.started":"2022-04-08T04:45:07.380434Z","shell.execute_reply":"2022-04-08T06:46:15.224874Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Обучение осуществлялось в несколько этапов, чтобы воспроизвести кривые обучения были сохранены словари с данными о функции потерь и точности по эпохам.","metadata":{"execution":{"iopub.status.busy":"2022-04-08T07:14:03.178773Z","iopub.execute_input":"2022-04-08T07:14:03.179055Z","iopub.status.idle":"2022-04-08T07:14:03.184112Z","shell.execute_reply.started":"2022-04-08T07:14:03.179025Z","shell.execute_reply":"2022-04-08T07:14:03.183159Z"}}},{"cell_type":"code","source":"with open('dct_history_4epoch.pkl', 'wb') as f:\n    pickle.dump(dct_history, f)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T06:49:09.757502Z","iopub.execute_input":"2022-04-08T06:49:09.757754Z","iopub.status.idle":"2022-04-08T06:49:09.762423Z","shell.execute_reply.started":"2022-04-08T06:49:09.757724Z","shell.execute_reply":"2022-04-08T06:49:09.761491Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"with open('../input/dct-history/dct_history_2epoch.pkl', \"rb\") as f:\n    dct_history1  = pickle.load(f)\nwith open('../input/dct-history/dct_history_3epoch.pkl', \"rb\") as f:\n    dct_history2  = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T07:10:04.468543Z","iopub.execute_input":"2022-04-08T07:10:04.469262Z","iopub.status.idle":"2022-04-08T07:10:04.477041Z","shell.execute_reply.started":"2022-04-08T07:10:04.469214Z","shell.execute_reply":"2022-04-08T07:10:04.476263Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"import copy\ndef mergeDict(dict1, dict2):\n    dct_e = copy.deepcopy(dict1)\n    for k, v in dict2.items():\n        dct_e[k].extend(v)\n    return dct_e","metadata":{"execution":{"iopub.status.busy":"2022-04-08T07:10:18.078007Z","iopub.execute_input":"2022-04-08T07:10:18.078282Z","iopub.status.idle":"2022-04-08T07:10:18.084393Z","shell.execute_reply.started":"2022-04-08T07:10:18.078253Z","shell.execute_reply":"2022-04-08T07:10:18.082763Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"dct_12 = mergeDict(dct_history1, dct_history2)\ndct_all = mergeDict(dct_12, dct_history)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T07:10:58.619955Z","iopub.execute_input":"2022-04-08T07:10:58.620487Z","iopub.status.idle":"2022-04-08T07:10:58.625602Z","shell.execute_reply.started":"2022-04-08T07:10:58.620450Z","shell.execute_reply":"2022-04-08T07:10:58.624697Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"learning_curves(dct_all['losses_train'], dct_all['losses_val'], txt='loss')","metadata":{"execution":{"iopub.status.busy":"2022-04-08T07:11:06.475483Z","iopub.execute_input":"2022-04-08T07:11:06.475746Z","iopub.status.idle":"2022-04-08T07:11:06.718462Z","shell.execute_reply.started":"2022-04-08T07:11:06.475717Z","shell.execute_reply":"2022-04-08T07:11:06.717699Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"learning_curves(dct_all['accuracy_train'], dct_all['accuracy_val'], txt='Accuracy')","metadata":{"execution":{"iopub.status.busy":"2022-04-08T07:11:07.304249Z","iopub.execute_input":"2022-04-08T07:11:07.304992Z","iopub.status.idle":"2022-04-08T07:11:07.515433Z","shell.execute_reply.started":"2022-04-08T07:11:07.304950Z","shell.execute_reply":"2022-04-08T07:11:07.514689Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"def predict(test_dataloader, model, criterion):\n    loss_test = []\n    y_test = []\n    y_predict = []\n    acc_test = []\n    m = nn.Softmax(dim=1)\n    model.eval()\n    with torch.no_grad():\n        optimizer.zero_grad()\n        for d_t in tqdm(test_dataloader):\n            X_t = d_t[0].unsqueeze(1).float().to(device)\n            y_t = d_t[1].to(device)\n            predict_t = model(X_t)\n            loss_t = criterion(predict_t, y_t)\n            logits_t = m(predict_t).detach().cpu().numpy().argmax(axis=-1).flatten().tolist()\n            loss_test.append(loss_t.item())\n            acc_test.append(accuracy_score(logits_t, y_t.cpu().numpy()))\n            y_test.extend(y_t.cpu().numpy())\n            y_predict.extend(logits_t)\n    print('---testing---')\n    print('Accuracy: {}, Loss: {}'.format(np.mean(acc_test), np.mean(loss_test)))\n    return y_test, y_predict","metadata":{"execution":{"iopub.status.busy":"2022-04-08T07:14:30.065488Z","iopub.execute_input":"2022-04-08T07:14:30.065776Z","iopub.status.idle":"2022-04-08T07:14:30.075980Z","shell.execute_reply.started":"2022-04-08T07:14:30.065744Z","shell.execute_reply":"2022-04-08T07:14:30.075110Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"Повторный прогон на валидационных данных для построения confusion_matrix, по которой достаточно сложно осуществлять какой-то детальный анализ, тем не менее это позволяет визуализировать точность работы модели (основные предсказания (корректные) на диагонали) ","metadata":{}},{"cell_type":"code","source":"y_test, y_predict = predict(val_dataloader, model, criterion)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T07:14:35.109291Z","iopub.execute_input":"2022-04-08T07:14:35.109547Z","iopub.status.idle":"2022-04-08T07:37:08.499994Z","shell.execute_reply.started":"2022-04-08T07:14:35.109519Z","shell.execute_reply":"2022-04-08T07:37:08.499251Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"# from sklearn.metrics import classification_report\n# print(classification_report(y_test, y_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ndef show_confusion_matrix(y_test, y_pred):\n    cm = confusion_matrix(y_test, y_pred)\n    df_cm = pd.DataFrame(cm)\n    plt.figure(figsize=(50,40))\n    hmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"Blues\", annot_kws={\"fontsize\":18})\n    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n    plt.ylabel('True sentiment', fontsize=18)\n    plt.xlabel('Predicted sentiment', fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T07:37:29.162972Z","iopub.execute_input":"2022-04-08T07:37:29.163308Z","iopub.status.idle":"2022-04-08T07:37:29.174760Z","shell.execute_reply.started":"2022-04-08T07:37:29.163269Z","shell.execute_reply":"2022-04-08T07:37:29.173862Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"show_confusion_matrix(y_test, y_predict)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T07:37:31.280322Z","iopub.execute_input":"2022-04-08T07:37:31.280599Z","iopub.status.idle":"2022-04-08T07:38:03.609176Z","shell.execute_reply.started":"2022-04-08T07:37:31.280568Z","shell.execute_reply":"2022-04-08T07:38:03.608201Z"},"trusted":true},"execution_count":63,"outputs":[]}]}