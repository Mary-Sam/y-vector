# y-vector

Статья, на основе которой сделано задание: https://www.isca-speech.org/archive/pdfs/interspeech_2021/zhu21b_interspeech.pdf. В статье описывается построение векторного представления (эмбеддингов) на сырых аудио данных (.wav).

Отличительной особенностью y-векторов от других подходов, основанных на извлечении признаков из сырых данных (например, таких как wav2vec), является то, что в данном случае используются три паралелльных канала со свёртками с разными ядрами, выполняющих функции фильтров разных частот. После свёрток по каналам  происходит частотное и временное сжатие-расширение в трёх последующих блоках. Для повышения точности происходит объединение карт признаков с различных слоёв для каждого фрейма данных.


Выбранный датасет для экспериментов - VCTK (https://datashare.ed.ac.uk/handle/10283/2950). VCTK включает в себя речевые данные, произнесенные 109 носителями английского языка с различными акцентами. В данном случае решалась задача идентификации, то есть классификация на 109 классов. При этом в исходной статье про y-вектора рассматривалась задача верификации, поэтому полученные результаты сравнивались с результатами из статьи по VCTK датасету.
Статья по VCTK: https://www.researchgate.net/publication/320280030_Dilated_Recurrent_Neural_Networks. 
В данной статье в качетстве метрики использовалась accuracy:
MFCC+GRU                   - 0.77
сырые данные + Dilited GRU - 0.74
сырые данные + Fused GRU   - 0.65

**параметры обучения:**
  - эпохи - 3
  - шаг обучения - 0.0001
  - батч - 64
  - функция потерь - CrossEntropyLoss
  - длина входящего фрагмента - 3.5с
Если входящий файл меньше 3.5с, то он дополняется до длины в 3.5с самим собой.

При разбиении данных 80/20 на валидационном множестве accuracy составляет 0.92, что является высоким показателем для классификации на 109 классов. Полученный результат выше, чем в представленной выше статье по задаче идентификации на VCTK корпусе.

Благодаря высокой точности при классификции данный подход можно использовать, например, в задаче диаризации для речевой аналитики команды из более 100 человек.
Возможные улучшения метода: исследовать применимость различных фильтров на начальных этапах преобразования данных, более сознательный подбор гиперпараметров


https://arxiv.org/abs/2204.01387
В статье рассказывается о модели для задачи голосового антиспуфинга, основанной на предварительно обученном wav2vec 2.0. Предварительно обученная модель wav2vec 2.0 извлекает векторное представление речи на основе эмбеддингов, которые уже получены из другой задачи. Данный подход позволяет повышать производительнсоть в условиях ограниченных ресурсов и разных наборов данных. Подход тестировался на данных с конкурса ASVspoof2019 трека LA (логический доступ), в котором атака совершается посредством TTS и VC. На данный момент эксперименты по статье я ставить не пробовала, но потенциально данный подход можно попробовать применить также для PA (физический доступ), в котором атака на систему верификации производится посредством повторного воспроизведения записи. 

.

**ссылка на ноутбук** - 


