# y-vector

Статья, на основе которой сделано задание: https://www.isca-speech.org/archive/pdfs/interspeech_2021/zhu21b_interspeech.pdf. В статье описывается построение векторного представления (эмбеддингов) на сырых аудио данных (.wav).

Отличительной особенностью y-векторов от других подходов, основанных на извлечении признаков из сырых данных (например, таких как wav2vec), является то, что в данном случае используются три паралелльных канала со свёртками с разными ядрами, выполняющих функции фильтров разных частот. После свёрток по каналам  происходит частотное и временное сжатие-расширение в трёх последующих блоках. 


Выбранный датасет для экспериментов - VCTK (https://datashare.ed.ac.uk/handle/10283/2950). VCTK включает в себя речевые данные, произнесенные 109 носителями английского языка с различными акцентами. В данном случае решалась задача идентификации, то есть классификация на 109 классов. При этом в исходной статье про y-вектора рассматривалась задача верификации, поэтому полученные результаты сравнивались с результатами из статьи про VCTK датасете
Статья по VCTK: https://www.researchgate.net/publication/359647053_Improved_Relation_Networks_for_End-to-End_Speaker_Verification_and_Identification



Поэтому для дальнейшего повышения точности мы объединяем карты объектов на разных слоях в каждом временном интервале. Для этого используется максимальный пул для понижения дискретизации карт объектов более раннего слоя до той же частоты кадров, что и у последнего слоя. 
